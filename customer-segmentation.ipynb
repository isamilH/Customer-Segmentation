{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3404,"sourceType":"datasetVersion","datasetId":1985}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-08T18:17:43.743504Z","iopub.execute_input":"2024-09-08T18:17:43.743993Z","iopub.status.idle":"2024-09-08T18:17:44.215673Z","shell.execute_reply.started":"2024-09-08T18:17:43.743939Z","shell.execute_reply":"2024-09-08T18:17:44.214332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Customer Segmentation**\nin this notebook i will apply K-Means algorithm is one of the centroid based clustering algorithms. Here k is the number of clusters and is a hyperparameter to the algorithm.\n","metadata":{}},{"cell_type":"markdown","source":"1. Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport plotly.graph_objects as go\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib import colors as mcolors\nfrom scipy.stats import linregress\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\nfrom sklearn.cluster import KMeans\nfrom tabulate import tabulate\nfrom collections import Counter\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:44.218193Z","iopub.execute_input":"2024-09-08T18:17:44.218895Z","iopub.status.idle":"2024-09-08T18:17:46.054930Z","shell.execute_reply.started":"2024-09-08T18:17:44.218839Z","shell.execute_reply":"2024-09-08T18:17:46.053781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ecommerce-data/data.csv', encoding=\"ISO-8859-1\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:46.056855Z","iopub.execute_input":"2024-09-08T18:17:46.057398Z","iopub.status.idle":"2024-09-08T18:17:47.670088Z","shell.execute_reply.started":"2024-09-08T18:17:46.057353Z","shell.execute_reply":"2024-09-08T18:17:47.668653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:47.673328Z","iopub.execute_input":"2024-09-08T18:17:47.673854Z","iopub.status.idle":"2024-09-08T18:17:47.711922Z","shell.execute_reply.started":"2024-09-08T18:17:47.673799Z","shell.execute_reply":"2024-09-08T18:17:47.710612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:47.713772Z","iopub.execute_input":"2024-09-08T18:17:47.714256Z","iopub.status.idle":"2024-09-08T18:17:48.028031Z","shell.execute_reply.started":"2024-09-08T18:17:47.714205Z","shell.execute_reply":"2024-09-08T18:17:48.026851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n* InvoiceNo: This is an object data type column that contains the invoice number for each transaction. Each invoice number can represent multiple items purchased in a single transaction.\n\n* StockCode: An object data type column representing the product code for each item.\n\n* Description: This column, also an object data type, contains descriptions of the products. It has some missing values, with 540,455 non-null entries out of 541,909.\n\n* Quantity: This is an integer column indicating the quantity of products purchased in each transaction.\n\n* InvoiceDate: A datetime column that records the date and time of each transaction.\n\n* UnitPrice: A float column representing the unit price of each product.\n\n* CustomerID: A float column that contains the customer ID for each transaction. This column has a significant number of missing values, with only 406,829 non-null entries out of 541,909.\n\n* Country: An object column recording the country where each transaction took place.\n","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:48.029640Z","iopub.execute_input":"2024-09-08T18:17:48.030042Z","iopub.status.idle":"2024-09-08T18:17:48.123145Z","shell.execute_reply.started":"2024-09-08T18:17:48.030002Z","shell.execute_reply":"2024-09-08T18:17:48.121878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":".The min in quantity is a negative value so it represents canceled orders.","metadata":{}},{"cell_type":"code","source":"df.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:48.124553Z","iopub.execute_input":"2024-09-08T18:17:48.124933Z","iopub.status.idle":"2024-09-08T18:17:48.945771Z","shell.execute_reply.started":"2024-09-08T18:17:48.124895Z","shell.execute_reply":"2024-09-08T18:17:48.944466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we noticed that some CustomerIDs are missing so we will drop them.","metadata":{}},{"cell_type":"code","source":"df = df.dropna(subset=['CustomerID', 'Description'])","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:48.947386Z","iopub.execute_input":"2024-09-08T18:17:48.947981Z","iopub.status.idle":"2024-09-08T18:17:49.062137Z","shell.execute_reply.started":"2024-09-08T18:17:48.947918Z","shell.execute_reply":"2024-09-08T18:17:49.060872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:49.064066Z","iopub.execute_input":"2024-09-08T18:17:49.064577Z","iopub.status.idle":"2024-09-08T18:17:49.291374Z","shell.execute_reply.started":"2024-09-08T18:17:49.064520Z","shell.execute_reply":"2024-09-08T18:17:49.290182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding duplicate rows","metadata":{}},{"cell_type":"code","source":"print(f\"The dataset contains {df.duplicated().sum()} duplicate rows that need to be removed.\")\n\n# Removing duplicate rows\ndf.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:49.297072Z","iopub.execute_input":"2024-09-08T18:17:49.297520Z","iopub.status.idle":"2024-09-08T18:17:49.981986Z","shell.execute_reply.started":"2024-09-08T18:17:49.297477Z","shell.execute_reply":"2024-09-08T18:17:49.980718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The dataset contains {df.duplicated().sum()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:49.983279Z","iopub.execute_input":"2024-09-08T18:17:49.983748Z","iopub.status.idle":"2024-09-08T18:17:50.268908Z","shell.execute_reply.started":"2024-09-08T18:17:49.983697Z","shell.execute_reply":"2024-09-08T18:17:50.267671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:50.270640Z","iopub.execute_input":"2024-09-08T18:17:50.271121Z","iopub.status.idle":"2024-09-08T18:17:50.499577Z","shell.execute_reply.started":"2024-09-08T18:17:50.271067Z","shell.execute_reply":"2024-09-08T18:17:50.498247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Transaction_Status'] = np.where(df['InvoiceNo'].astype(str).str.startswith('C'), 'Cancelled', 'Completed')\n\n# Analyze the characteristics of these rows (considering the new column)\ncancelled_transactions = df[df['Transaction_Status'] == 'Cancelled']\ncancelled_transactions.describe().drop('CustomerID', axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:50.501023Z","iopub.execute_input":"2024-09-08T18:17:50.501402Z","iopub.status.idle":"2024-09-08T18:17:50.854242Z","shell.execute_reply.started":"2024-09-08T18:17:50.501365Z","shell.execute_reply":"2024-09-08T18:17:50.853076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cancelled_transactions\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:50.856258Z","iopub.execute_input":"2024-09-08T18:17:50.856747Z","iopub.status.idle":"2024-09-08T18:17:50.878996Z","shell.execute_reply.started":"2024-09-08T18:17:50.856694Z","shell.execute_reply":"2024-09-08T18:17:50.877489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:50.880771Z","iopub.execute_input":"2024-09-08T18:17:50.881318Z","iopub.status.idle":"2024-09-08T18:17:50.904076Z","shell.execute_reply.started":"2024-09-08T18:17:50.881252Z","shell.execute_reply":"2024-09-08T18:17:50.902651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Description Column treatment**","metadata":{}},{"cell_type":"code","source":"df['Description'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:50.905816Z","iopub.execute_input":"2024-09-08T18:17:50.906738Z","iopub.status.idle":"2024-09-08T18:17:50.997314Z","shell.execute_reply.started":"2024-09-08T18:17:50.906577Z","shell.execute_reply":"2024-09-08T18:17:50.996041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product_desc=df['Description'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:50.998883Z","iopub.execute_input":"2024-09-08T18:17:50.999259Z","iopub.status.idle":"2024-09-08T18:17:51.088467Z","shell.execute_reply.started":"2024-09-08T18:17:50.999220Z","shell.execute_reply":"2024-09-08T18:17:51.087052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntop_10_desc = product_desc[:30]\n\n\nplt.figure(figsize=(12,8))\nplt.barh(top_10_desc.index[::-1], top_10_desc.values[::-1])\n\nplt.xlabel('Number of instances')\nplt.ylabel('Description')\nplt.title('Top 10 Most Descriptions')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:51.090689Z","iopub.execute_input":"2024-09-08T18:17:51.091147Z","iopub.status.idle":"2024-09-08T18:17:51.804569Z","shell.execute_reply.started":"2024-09-08T18:17:51.091101Z","shell.execute_reply":"2024-09-08T18:17:51.803217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:51.806009Z","iopub.execute_input":"2024-09-08T18:17:51.806375Z","iopub.status.idle":"2024-09-08T18:17:51.867362Z","shell.execute_reply.started":"2024-09-08T18:17:51.806337Z","shell.execute_reply":"2024-09-08T18:17:51.866048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the unit price of 0 might be a data entry error so we will drop them ","metadata":{}},{"cell_type":"code","source":"df = df[df['UnitPrice'] > 0]","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:51.868858Z","iopub.execute_input":"2024-09-08T18:17:51.869216Z","iopub.status.idle":"2024-09-08T18:17:51.916913Z","shell.execute_reply.started":"2024-09-08T18:17:51.869180Z","shell.execute_reply":"2024-09-08T18:17:51.915689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now after the data processing let's do a full eda with the help of pandas_profiling","metadata":{}},{"cell_type":"code","source":"from pandas_profiling import ProfileReport\n\n#\n\n# Generate a profile report\nprofile = ProfileReport(df, title=\"Customer Segmentation\", explorative=True)\n\n# View the report in a Jupyter notebook\nprofile.to_notebook_iframe()\n\n# Or export it to an HTML file\nprofile.to_file(\"output_report.html\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:17:51.918313Z","iopub.execute_input":"2024-09-08T18:17:51.918855Z","iopub.status.idle":"2024-09-08T18:18:40.302218Z","shell.execute_reply.started":"2024-09-08T18:17:51.918800Z","shell.execute_reply":"2024-09-08T18:18:40.300846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RFM is a method used for analyzing customer value and segmenting the customer base. It is an acronym that stands for:\n\n*     Recency (R): This metric indicates how recently a customer has made a purchase. A lower recency value means the customer has purchased more recently, indicating higher engagement with the brand.\n\n*     Frequency (F): This metric signifies how often a customer makes a purchase within a certain period. A higher frequency value indicates a customer who interacts with the business more often, suggesting higher loyalty or satisfaction.\n\n*     Monetary (M): This metric represents the total amount of money a customer has spent over a certain period. Customers who have a higher monetary value have contributed more to the business, indicating their potential high lifetime value.\n","metadata":{}},{"cell_type":"code","source":"df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:25:55.522698Z","iopub.execute_input":"2024-09-08T18:25:55.523770Z","iopub.status.idle":"2024-09-08T18:25:55.738849Z","shell.execute_reply.started":"2024-09-08T18:25:55.523712Z","shell.execute_reply":"2024-09-08T18:25:55.737511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:25:59.815141Z","iopub.execute_input":"2024-09-08T18:25:59.815568Z","iopub.status.idle":"2024-09-08T18:25:59.838526Z","shell.execute_reply.started":"2024-09-08T18:25:59.815530Z","shell.execute_reply":"2024-09-08T18:25:59.837146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Engineering**\n* RFM Features\n* Product Diversity\n* Geographic Features","metadata":{}},{"cell_type":"code","source":"# Find the most recent purchase date for each customer\ncustomer_data = df.groupby('CustomerID')['InvoiceDate'].max().reset_index()\n\n# Find the most recent date in the entire dataset\nmost_recent_date = df['InvoiceDate'].max()\n\n# Convert InvoiceDay to datetime type before subtraction\ncustomer_data['InvoiceDate'] = pd.to_datetime(customer_data['InvoiceDate'])\nmost_recent_date = pd.to_datetime(most_recent_date)\n\n# Calculate the number of days since the last purchase for each customer\ncustomer_data['Days_Since_Last_Purchase'] = (most_recent_date - customer_data['InvoiceDate']).dt.days\n\n# Remove the InvoiceDay column\ncustomer_data.drop(columns=['InvoiceDate'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:50:36.756454Z","iopub.execute_input":"2024-09-08T18:50:36.756958Z","iopub.status.idle":"2024-09-08T18:50:36.795343Z","shell.execute_reply.started":"2024-09-08T18:50:36.756914Z","shell.execute_reply":"2024-09-08T18:50:36.794285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:50:45.904981Z","iopub.execute_input":"2024-09-08T18:50:45.905451Z","iopub.status.idle":"2024-09-08T18:50:45.919968Z","shell.execute_reply.started":"2024-09-08T18:50:45.905403Z","shell.execute_reply":"2024-09-08T18:50:45.918677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_transactions = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\ntotal_transactions.rename(columns={'InvoiceNo': 'Total_Transactions'}, inplace=True)\n\n# Calculate the total number of products purchased by each customer\ntotal_products_purchased = df.groupby('CustomerID')['Quantity'].sum().reset_index()\ntotal_products_purchased.rename(columns={'Quantity': 'Total_Products_Purchased'}, inplace=True)\n\n# Merge the new features into the customer_data dataframe\ncustomer_data = pd.merge(customer_data, total_transactions, on='CustomerID')\ncustomer_data = pd.merge(customer_data, total_products_purchased, on='CustomerID')","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:51:22.313459Z","iopub.execute_input":"2024-09-08T18:51:22.314357Z","iopub.status.idle":"2024-09-08T18:51:22.400667Z","shell.execute_reply.started":"2024-09-08T18:51:22.314307Z","shell.execute_reply":"2024-09-08T18:51:22.399424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Total_Spend'] = df['UnitPrice'] * df['Quantity']\ntotal_spend = df.groupby('CustomerID')['Total_Spend'].sum().reset_index()\n\n# Calculate the average transaction value for each customer\naverage_transaction_value = total_spend.merge(total_transactions, on='CustomerID')\naverage_transaction_value['Average_Transaction_Value'] = average_transaction_value['Total_Spend'] / average_transaction_value['Total_Transactions']\n\n# Merge the new features into the customer_data dataframe\ncustomer_data = pd.merge(customer_data, total_spend, on='CustomerID')\ncustomer_data = pd.merge(customer_data, average_transaction_value[['CustomerID', 'Average_Transaction_Value']], on='CustomerID')\n\n# Display the first few rows of the customer_data dataframe\ncustomer_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:51:41.271981Z","iopub.execute_input":"2024-09-08T18:51:41.272434Z","iopub.status.idle":"2024-09-08T18:51:41.324810Z","shell.execute_reply.started":"2024-09-08T18:51:41.272392Z","shell.execute_reply":"2024-09-08T18:51:41.323474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the number of unique products purchased by each customer\nunique_products_purchased = df.groupby('CustomerID')['StockCode'].nunique().reset_index()\nunique_products_purchased.rename(columns={'StockCode': 'Unique_Products_Purchased'}, inplace=True)\n\n# Merge the new feature into the customer_data dataframe\ncustomer_data = pd.merge(customer_data, unique_products_purchased, on='CustomerID')\n\n# Display the first few rows of the customer_data dataframe\ncustomer_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:52:06.579738Z","iopub.execute_input":"2024-09-08T18:52:06.580208Z","iopub.status.idle":"2024-09-08T18:52:06.665972Z","shell.execute_reply.started":"2024-09-08T18:52:06.580166Z","shell.execute_reply":"2024-09-08T18:52:06.664765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by CustomerID and Country to get the number of transactions per country for each customer\ncustomer_country = df.groupby(['CustomerID', 'Country']).size().reset_index(name='Number_of_Transactions')\n\n# Get the country with the maximum number of transactions for each customer (in case a customer has transactions from multiple countries)\ncustomer_main_country = customer_country.sort_values('Number_of_Transactions', ascending=False).drop_duplicates('CustomerID')\n\n# Create a binary column indicating whether the customer is from the UK or not\ncustomer_main_country['Is_UK'] = customer_main_country['Country'].apply(lambda x: 1 if x == 'United Kingdom' else 0)\n\n# Merge this data with our customer_data dataframe\ncustomer_data = pd.merge(customer_data, customer_main_country[['CustomerID', 'Is_UK']], on='CustomerID', how='left')\n\n# Display the first few rows of the customer_data dataframe\ncustomer_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:52:27.832955Z","iopub.execute_input":"2024-09-08T18:52:27.833842Z","iopub.status.idle":"2024-09-08T18:52:27.923916Z","shell.execute_reply.started":"2024-09-08T18:52:27.833796Z","shell.execute_reply":"2024-09-08T18:52:27.922708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a profile report\nprofile = ProfileReport(customer_data, title=\"Customer Segmentation\", explorative=True)\n\n# View the report in a Jupyter notebook\nprofile.to_notebook_iframe()\n\n# Or export it to an HTML file\nprofile.to_file(\"output_report.html\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:53:49.255860Z","iopub.execute_input":"2024-09-08T18:53:49.256359Z","iopub.status.idle":"2024-09-08T18:54:13.769098Z","shell.execute_reply.started":"2024-09-08T18:53:49.256316Z","shell.execute_reply":"2024-09-08T18:54:13.768004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# List of columns that don't need to be scaled\ncolumns_to_exclude = ['CustomerID', 'Is_UK', 'Day_Of_Week']\n\n# List of columns that need to be scaled\ncolumns_to_scale = customer_data.columns.difference(columns_to_exclude)\n\n# Copy the cleaned dataset\ncustomer_data_scaled = customer_data.copy()\n\n# Applying the scaler to the necessary columns in the dataset\ncustomer_data_scaled[columns_to_scale] = scaler.fit_transform(customer_data_scaled[columns_to_scale])\n\n# Display the first few rows of the scaled data\ncustomer_data_scaled.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:55:50.402881Z","iopub.execute_input":"2024-09-08T18:55:50.403646Z","iopub.status.idle":"2024-09-08T18:55:50.430070Z","shell.execute_reply.started":"2024-09-08T18:55:50.403587Z","shell.execute_reply":"2024-09-08T18:55:50.428882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting CustomerID as the index column\ncustomer_data_scaled.set_index('CustomerID', inplace=True)\n\n# Apply PCA\npca = PCA().fit(customer_data_scaled)\n\n# Calculate the Cumulative Sum of the Explained Variance\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_explained_variance = np.cumsum(explained_variance_ratio)\n\n# Set the optimal k value (based on our analysis, we can choose 6)\noptimal_k = 6\n\n# Set seaborn plot style\nsns.set(rc={'axes.facecolor': '#fcf0dc'}, style='darkgrid')\n\n# Plot the cumulative explained variance against the number of components\nplt.figure(figsize=(20, 10))\n\n# Bar chart for the explained variance of each component\nbarplot = sns.barplot(x=list(range(1, len(cumulative_explained_variance) + 1)),\n                      y=explained_variance_ratio,\n                      color='#fcc36d',\n                      alpha=0.8)\n\n# Line plot for the cumulative explained variance\nlineplot, = plt.plot(range(0, len(cumulative_explained_variance)), cumulative_explained_variance,\n                     marker='o', linestyle='--', color='#ff6200', linewidth=2)\n\n# Plot optimal k value line\noptimal_k_line = plt.axvline(optimal_k - 1, color='red', linestyle='--', label=f'Optimal k value = {optimal_k}') \n\n# Set labels and title\nplt.xlabel('Number of Components', fontsize=14)\nplt.ylabel('Explained Variance', fontsize=14)\nplt.title('Cumulative Variance vs. Number of Components', fontsize=18)\n\n# Customize ticks and legend\nplt.xticks(range(0, len(cumulative_explained_variance)))\nplt.legend(handles=[barplot.patches[0], lineplot, optimal_k_line],\n           labels=['Explained Variance of Each Component', 'Cumulative Explained Variance', f'Optimal k value = {optimal_k}'],\n           loc=(0.62, 0.1),\n           frameon=True,\n           framealpha=1.0,  \n           edgecolor='#ff6200')  \n\n# Display the variance values for both graphs on the plots\nx_offset = -0.3\ny_offset = 0.01\nfor i, (ev_ratio, cum_ev_ratio) in enumerate(zip(explained_variance_ratio, cumulative_explained_variance)):\n    plt.text(i, ev_ratio, f\"{ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n    if i > 0:\n        plt.text(i + x_offset, cum_ev_ratio + y_offset, f\"{cum_ev_ratio:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n\nplt.grid(axis='both')   \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:56:20.759921Z","iopub.execute_input":"2024-09-08T18:56:20.760893Z","iopub.status.idle":"2024-09-08T18:56:21.373164Z","shell.execute_reply.started":"2024-09-08T18:56:20.760844Z","shell.execute_reply":"2024-09-08T18:56:21.371885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a PCA object with 6 components\npca = PCA(n_components=6)\n\n# Fitting and transforming the original data to the new PCA dataframe\ncustomer_data_pca = pca.fit_transform(customer_data_scaled)\n\n# Creating a new dataframe from the PCA dataframe, with columns labeled PC1, PC2, etc.\ncustomer_data_pca = pd.DataFrame(customer_data_pca, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n\n# Adding the CustomerID index back to the new PCA dataframe\ncustomer_data_pca.index = customer_data_scaled.index\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:56:49.763222Z","iopub.execute_input":"2024-09-08T18:56:49.763863Z","iopub.status.idle":"2024-09-08T18:56:49.778687Z","shell.execute_reply.started":"2024-09-08T18:56:49.763803Z","shell.execute_reply":"2024-09-08T18:56:49.777240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data_pca.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:56:57.878826Z","iopub.execute_input":"2024-09-08T18:56:57.879390Z","iopub.status.idle":"2024-09-08T18:56:57.899304Z","shell.execute_reply.started":"2024-09-08T18:56:57.879335Z","shell.execute_reply":"2024-09-08T18:56:57.897855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to highlight the top 3 absolute values in each column of a dataframe\ndef highlight_top3(column):\n    top3 = column.abs().nlargest(3).index\n    return ['background-color:  #ffeacc' if i in top3 else '' for i in column.index]\n\n# Create the PCA component DataFrame and apply the highlighting function\npc_df = pd.DataFrame(pca.components_.T, columns=['PC{}'.format(i+1) for i in range(pca.n_components_)],  \n                     index=customer_data_scaled.columns)\n\npc_df.style.apply(highlight_top3, axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:57:14.507825Z","iopub.execute_input":"2024-09-08T18:57:14.508292Z","iopub.status.idle":"2024-09-08T18:57:14.593037Z","shell.execute_reply.started":"2024-09-08T18:57:14.508251Z","shell.execute_reply":"2024-09-08T18:57:14.591671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set plot style, and background color\nsns.set(style='darkgrid', rc={'axes.facecolor': '#fcf0dc'})\n\n# Set the color palette for the plot\nsns.set_palette(['#ff6200'])\n\n# Instantiate the clustering model with the specified parameters\nkm = KMeans(init='k-means++', n_init=10, max_iter=100, random_state=0)\n\n# Create a figure and axis with the desired size\nfig, ax = plt.subplots(figsize=(12, 5))\n\n# Instantiate the KElbowVisualizer with the model and range of k values, and disable the timing plot\nvisualizer = KElbowVisualizer(km, k=(2, 15), timings=False, ax=ax)\n\n# Fit the data to the visualizer\nvisualizer.fit(customer_data_pca)\n\n# Finalize and render the figure\nvisualizer.show();","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:57:37.374392Z","iopub.execute_input":"2024-09-08T18:57:37.374957Z","iopub.status.idle":"2024-09-08T18:57:55.064462Z","shell.execute_reply.started":"2024-09-08T18:57:37.374900Z","shell.execute_reply":"2024-09-08T18:57:55.063126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def silhouette_analysis(df, start_k, stop_k, figsize=(15, 16)):\n    \"\"\"\n    Perform Silhouette analysis for a range of k values and visualize the results.\n    \"\"\"\n\n    # Set the size of the figure\n    plt.figure(figsize=figsize)\n\n    # Create a grid with (stop_k - start_k + 1) rows and 2 columns\n    grid = gridspec.GridSpec(stop_k - start_k + 1, 2)\n\n    # Assign the first plot to the first row and both columns\n    first_plot = plt.subplot(grid[0, :])\n\n    # First plot: Silhouette scores for different k values\n    sns.set_palette(['darkorange'])\n\n    silhouette_scores = []\n\n    # Iterate through the range of k values\n    for k in range(start_k, stop_k + 1):\n        km = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=100, random_state=0)\n        km.fit(df)\n        labels = km.predict(df)\n        score = silhouette_score(df, labels)\n        silhouette_scores.append(score)\n\n    best_k = start_k + silhouette_scores.index(max(silhouette_scores))\n\n    plt.plot(range(start_k, stop_k + 1), silhouette_scores, marker='o')\n    plt.xticks(range(start_k, stop_k + 1))\n    plt.xlabel('Number of clusters (k)')\n    plt.ylabel('Silhouette score')\n    plt.title('Average Silhouette Score for Different k Values', fontsize=15)\n\n    # Add the optimal k value text to the plot\n    optimal_k_text = f'The k value with the highest Silhouette score is: {best_k}'\n    plt.text(10, 0.23, optimal_k_text, fontsize=12, verticalalignment='bottom', \n             horizontalalignment='left', bbox=dict(facecolor='#fcc36d', edgecolor='#ff6200', boxstyle='round, pad=0.5'))\n             \n\n    # Second plot (subplot): Silhouette plots for each k value\n    colors = sns.color_palette(\"bright\")\n\n    for i in range(start_k, stop_k + 1):    \n        km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=0)\n        row_idx, col_idx = divmod(i - start_k, 2)\n\n        # Assign the plots to the second, third, and fourth rows\n        ax = plt.subplot(grid[row_idx + 1, col_idx])\n\n        visualizer = SilhouetteVisualizer(km, colors=colors, ax=ax)\n        visualizer.fit(df)\n\n        # Add the Silhouette score text to the plot\n        score = silhouette_score(df, km.labels_)\n        ax.text(0.97, 0.02, f'Silhouette Score: {score:.2f}', fontsize=12, \\\n                ha='right', transform=ax.transAxes, color='red')\n\n        ax.set_title(f'Silhouette Plot for {i} Clusters', fontsize=15)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:58:55.704700Z","iopub.execute_input":"2024-09-08T18:58:55.705837Z","iopub.status.idle":"2024-09-08T18:58:55.722777Z","shell.execute_reply.started":"2024-09-08T18:58:55.705773Z","shell.execute_reply":"2024-09-08T18:58:55.721447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"silhouette_analysis(customer_data_pca, 3, 12, figsize=(20, 50))","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:59:05.056634Z","iopub.execute_input":"2024-09-08T18:59:05.057451Z","iopub.status.idle":"2024-09-08T18:59:51.258926Z","shell.execute_reply.started":"2024-09-08T18:59:05.057402Z","shell.execute_reply":"2024-09-08T18:59:51.257574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply KMeans clustering using the optimal k\nkmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)\nkmeans.fit(customer_data_pca)\n\n# Get the frequency of each cluster\ncluster_frequencies = Counter(kmeans.labels_)\n\n# Create a mapping from old labels to new labels based on frequency\nlabel_mapping = {label: new_label for new_label, (label, _) in \n                 enumerate(cluster_frequencies.most_common())}\n\n# Reverse the mapping to assign labels as per your criteria\nlabel_mapping = {v: k for k, v in {2: 1, 1: 0, 0: 2}.items()}\n\n# Apply the mapping to get the new labels\nnew_labels = np.array([label_mapping[label] for label in kmeans.labels_])\n\n# Append the new cluster labels back to the original dataset\ncustomer_data['cluster'] = new_labels\n\n# Append the new cluster labels to the PCA version of the dataset\ncustomer_data_pca['cluster'] = new_labels","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:00:09.886211Z","iopub.execute_input":"2024-09-08T19:00:09.886718Z","iopub.status.idle":"2024-09-08T19:00:10.995336Z","shell.execute_reply.started":"2024-09-08T19:00:09.886666Z","shell.execute_reply":"2024-09-08T19:00:10.994088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = ['#e8000b', '#1ac938', '#023eff']","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:00:40.947994Z","iopub.execute_input":"2024-09-08T19:00:40.948973Z","iopub.status.idle":"2024-09-08T19:00:40.954355Z","shell.execute_reply.started":"2024-09-08T19:00:40.948924Z","shell.execute_reply":"2024-09-08T19:00:40.952963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create separate data frames for each cluster\ncluster_0 = customer_data_pca[customer_data_pca['cluster'] == 0]\ncluster_1 = customer_data_pca[customer_data_pca['cluster'] == 1]\ncluster_2 = customer_data_pca[customer_data_pca['cluster'] == 2]\n\n# Create a 3D scatter plot\nfig = go.Figure()\n\n# Add data points for each cluster separately and specify the color\nfig.add_trace(go.Scatter3d(x=cluster_0['PC1'], y=cluster_0['PC2'], z=cluster_0['PC3'], \n                           mode='markers', marker=dict(color=colors[0], size=5, opacity=0.4), name='Cluster 0'))\nfig.add_trace(go.Scatter3d(x=cluster_1['PC1'], y=cluster_1['PC2'], z=cluster_1['PC3'], \n                           mode='markers', marker=dict(color=colors[1], size=5, opacity=0.4), name='Cluster 1'))\nfig.add_trace(go.Scatter3d(x=cluster_2['PC1'], y=cluster_2['PC2'], z=cluster_2['PC3'], \n                           mode='markers', marker=dict(color=colors[2], size=5, opacity=0.4), name='Cluster 2'))\n\n# Set the title and layout details\nfig.update_layout(\n    title=dict(text='3D Visualization of Customer Clusters in PCA Space', x=0.5),\n    scene=dict(\n        xaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC1'),\n        yaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC2'),\n        zaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white', title='PC3'),\n    ),\n    width=900,\n    height=800\n)\n\n# Show the plot\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:00:55.511667Z","iopub.execute_input":"2024-09-08T19:00:55.512132Z","iopub.status.idle":"2024-09-08T19:00:56.147605Z","shell.execute_reply.started":"2024-09-08T19:00:55.512092Z","shell.execute_reply":"2024-09-08T19:00:56.146456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the percentage of customers in each cluster\ncluster_percentage = (customer_data_pca['cluster'].value_counts(normalize=True) * 100).reset_index()\ncluster_percentage.columns = ['Cluster', 'Percentage']\ncluster_percentage.sort_values(by='Cluster', inplace=True)\n\n# Create a horizontal bar plot\nplt.figure(figsize=(10, 4))\nsns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h', palette=colors)\n\n# Adding percentages on the bars\nfor index, value in enumerate(cluster_percentage['Percentage']):\n    plt.text(value+0.5, index, f'{value:.2f}%')\n\nplt.title('Distribution of Customers Across Clusters', fontsize=14)\nplt.xticks(ticks=np.arange(0, 50, 5))\nplt.xlabel('Percentage (%)')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:01:44.750955Z","iopub.execute_input":"2024-09-08T19:01:44.751473Z","iopub.status.idle":"2024-09-08T19:01:45.075977Z","shell.execute_reply.started":"2024-09-08T19:01:44.751419Z","shell.execute_reply":"2024-09-08T19:01:45.074675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute number of customers\nnum_observations = len(customer_data_pca)\n\n# Separate the features and the cluster labels\nX = customer_data_pca.drop('cluster', axis=1)\nclusters = customer_data_pca['cluster']\n\n# Compute the metrics\nsil_score = silhouette_score(X, clusters)\ncalinski_score = calinski_harabasz_score(X, clusters)\ndavies_score = davies_bouldin_score(X, clusters)\n\n# Create a table to display the metrics and the number of observations\ntable_data = [\n    [\"Number of Observations\", num_observations],\n    [\"Silhouette Score\", sil_score],\n    [\"Calinski Harabasz Score\", calinski_score],\n    [\"Davies Bouldin Score\", davies_score]\n]\n\n# Print the table\nprint(tabulate(table_data, headers=[\"Metric\", \"Value\"], tablefmt='pretty'))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:02:06.827859Z","iopub.execute_input":"2024-09-08T19:02:06.828306Z","iopub.status.idle":"2024-09-08T19:02:07.202756Z","shell.execute_reply.started":"2024-09-08T19:02:06.828263Z","shell.execute_reply":"2024-09-08T19:02:07.201150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting 'CustomerID' column as index and assigning it to a new dataframe\ndf_customer = customer_data.set_index('CustomerID')\n\n# Standardize the data (excluding the cluster column)\nscaler = StandardScaler()\ndf_customer_standardized = scaler.fit_transform(df_customer.drop(columns=['cluster'], axis=1))\n\n# Create a new dataframe with standardized values and add the cluster column back\ndf_customer_standardized = pd.DataFrame(df_customer_standardized, columns=df_customer.columns[:-1], index=df_customer.index)\ndf_customer_standardized['cluster'] = df_customer['cluster']\n\n# Calculate the centroids of each cluster\ncluster_centroids = df_customer_standardized.groupby('cluster').mean()\n\n# Function to create a radar chart\ndef create_radar_chart(ax, angles, data, color, cluster):\n    # Plot the data and fill the area\n    ax.fill(angles, data, color=color, alpha=0.4)\n    ax.plot(angles, data, color=color, linewidth=2, linestyle='solid')\n    \n    # Add a title\n    ax.set_title(f'Cluster {cluster}', size=20, color=color, y=1.1)\n\n# Set data\nlabels=np.array(cluster_centroids.columns)\nnum_vars = len(labels)\n\n# Compute angle of each axis\nangles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n\n# The plot is circular, so we need to \"complete the loop\" and append the start to the end\nlabels = np.concatenate((labels, [labels[0]]))\nangles += angles[:1]\n\n# Initialize the figure\nfig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(polar=True), nrows=1, ncols=3)\n\n# Create radar chart for each cluster\nfor i, color in enumerate(colors):\n    data = cluster_centroids.loc[i].tolist()\n    data += data[:1]  # Complete the loop\n    create_radar_chart(ax[i], angles, data, color, i)\n\n# Add input data\nax[0].set_xticks(angles[:-1])\nax[0].set_xticklabels(labels[:-1])\n\nax[1].set_xticks(angles[:-1])\nax[1].set_xticklabels(labels[:-1])\n\nax[2].set_xticks(angles[:-1])\nax[2].set_xticklabels(labels[:-1])\n\n# Add a grid\nax[0].grid(color='grey', linewidth=0.5)\n\n# Display the plot\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:03:01.023095Z","iopub.execute_input":"2024-09-08T19:03:01.024045Z","iopub.status.idle":"2024-09-08T19:03:02.218784Z","shell.execute_reply.started":"2024-09-08T19:03:01.023982Z","shell.execute_reply":"2024-09-08T19:03:02.217366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot histograms for each feature segmented by the clusters\nfeatures = customer_data.columns[1:-1]\nclusters = customer_data['cluster'].unique()\nclusters.sort()\n\n# Setting up the subplots\nn_rows = len(features)\nn_cols = len(clusters)\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 3*n_rows))\n\n# Plotting histograms\nfor i, feature in enumerate(features):\n    for j, cluster in enumerate(clusters):\n        data = customer_data[customer_data['cluster'] == cluster][feature]\n        axes[i, j].hist(data, bins=20, color=colors[j], edgecolor='w', alpha=0.7)\n        axes[i, j].set_title(f'Cluster {cluster} - {feature}', fontsize=15)\n        axes[i, j].set_xlabel('')\n        axes[i, j].set_ylabel('')\n\n# Adjusting layout to prevent overlapping\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:04:07.759075Z","iopub.execute_input":"2024-09-08T19:04:07.759490Z","iopub.status.idle":"2024-09-08T19:04:15.479057Z","shell.execute_reply.started":"2024-09-08T19:04:07.759452Z","shell.execute_reply":"2024-09-08T19:04:15.477894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}